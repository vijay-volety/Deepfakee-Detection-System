# Training Configuration for DeepFake Detection Model

# Model Architecture
model:
  type: resnet_lstm  # resnet_lstm, resnext_lstm
  resnet_type: "resnet50"  # resnet18, resnet34, resnet50, resnet101
  resnext_type: "resnext50_32x4d"  # resnext50_32x4d, resnext101_32x8d
  lstm_hidden_size: 512
  lstm_num_layers: 2
  num_classes: 2
  dropout_rate: 0.5
  freeze_resnet: false
  freeze_resnext: false

# Data Configuration
data:
  root_dir: "./data"  # Root directory containing datasets
  datasets: ["DFDC", "FaceForensics++", "Celeb-DF", "Archive"]  # Added Archive dataset
  sequence_length: 16  # Number of frames per sequence
  batch_size: 8  # Adjust based on GPU memory
  num_workers: 4
  
# Training Configuration
training:
  num_epochs: 150  # Increased epochs for better training with archive data
  early_stopping_patience: 20  # Increased patience
  early_stopping_min_delta: 0.001
  gradient_clipping: true
  max_grad_norm: 1.0
  save_freq: 10  # Save checkpoint every N epochs

# Optimizer Configuration
optimizer:
  type: "adam"  # adam, sgd
  learning_rate: 0.0001
  weight_decay: 0.0001

# Scheduler Configuration
scheduler:
  type: "reduce_on_plateau"  # step, cosine, reduce_on_plateau
  factor: 0.5
  patience: 10
  # For step scheduler:
  # step_size: 30
  # gamma: 0.1
  # For cosine scheduler:
  # T_max: 100

# Loss Configuration
loss:
  type: "cross_entropy"  # cross_entropy, focal
  # For class imbalance, add class weights:
  # class_weights: [1.0, 1.5]  # [authentic, deepfake]
  # For focal loss:
  # alpha: 1.0
  # gamma: 2.0

# Paths
paths:
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
  output_dir: "./outputs"

# Weights & Biases Configuration (optional)
use_wandb: false
wandb:
  project: "deepfake-detection"
  run_name: "resnet50-lstm-archive-enhanced"

# Reproducibility
seed: 42